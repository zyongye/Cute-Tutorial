{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82bad85",
   "metadata": {},
   "source": [
    "# Cute Layout Basics and Elementwise add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e833b5",
   "metadata": {},
   "source": [
    "Tensors are the core object in todays machine learning workload. In recent GPU libraries, this concept has been incorporate in the kernel level. We are going to introduce the core concept in Cute programming, the CuTe Layout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ee86f5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!export PYTHONUNBUFFERED=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ddc26e",
   "metadata": {},
   "source": [
    "## Layout\n",
    "Layout is a function that maps coordinate space (represented by integer or integer tuple) to index space (integers).\n",
    "We can think it as mapping multi-dimension index into a single integer index. \n",
    "\n",
    "\n",
    "We define a Layout as\n",
    "```math\n",
    "L = (M_0, M_1, \\dots, M_n):(d_0, d_1,\\dots,d_n)\n",
    "```\n",
    "here \n",
    "\n",
    "$S=(M_0, M_1,\\dots,M_n)$ is the shape where $M=M_0\\cdot\\dots\\cdot M_1$ is the size of the layout. \n",
    "\n",
    "Then given a multi-dimension index $\\iota=(\\iota_0, \\dots, \\iota_n)$, $L$ maps it into a single integer offset as \n",
    "```math\n",
    "L(\\iota)=\\iota_0d_0+\\iota_1d_1+\\cdots+\\iota_nd_n\n",
    "```\n",
    "We can easily observe that we can use layout function to generate offset that indexed into a 1D array with a base pointer, which is usually how tensor is represented in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23e42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cutlass\n",
    "import cutlass.cute as cute\n",
    "import torch\n",
    "\n",
    "from cutlass.utils import print_latex\n",
    "from cutlass.cute.runtime import from_dlpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.jit\n",
    "def create_layout1():\n",
    "    S = (2, 5)\n",
    "    D = (5, 1)\n",
    "    layout = cute.make_layout(S, stride=D)\n",
    "    print(layout)\n",
    "    for i in cutlass.range_constexpr(S[0]):\n",
    "        for j in cutlass.range_constexpr(S[1]):\n",
    "            cute.printf(\"fL({}, {}) = {}\", i, j, layout((i, j)))\n",
    "    # print_latex(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a30c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,5):(5,1)\n",
      "fL(0, 0) = 0\n",
      "fL(0, 1) = 1\n",
      "fL(0, 2) = 2\n",
      "fL(0, 3) = 3\n",
      "fL(0, 4) = 4\n",
      "fL(1, 0) = 5\n",
      "fL(1, 1) = 6\n",
      "fL(1, 2) = 7\n",
      "fL(1, 3) = 8\n",
      "fL(1, 4) = 9\n"
     ]
    }
   ],
   "source": [
    "create_layout1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f03b4",
   "metadata": {},
   "source": [
    "![alt text](figs/layout1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822306c7",
   "metadata": {},
   "source": [
    "And a single doesn't necessarily represent the entire matrix. \n",
    "\n",
    "For example we can have $(2, 5): (10, 1)$, which represent a subset of a bigger tensor.\n",
    "\n",
    "![alt text](figs/layout3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629cff34",
   "metadata": {},
   "source": [
    "$(M_i): (d_i)$ are called mode and they can just be a single integer or a layout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1b69c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,(2,2)):(4,(2,1))\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def create_layout2():\n",
    "    S = (2, (2, 2))\n",
    "    D = (4, (2, 1))\n",
    "    layout = cute.make_layout(S, stride=D)\n",
    "    print(layout)\n",
    "    # print_latex(layout)\n",
    "create_layout2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee751c97",
   "metadata": {},
   "source": [
    "![alt text](figs/layout2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16aeae9",
   "metadata": {},
   "source": [
    "This layout can be indexed in all 1D, 2D and 3D coordinate.\n",
    "\n",
    "The rule for converting 1D layout into natural layout is the following rules\n",
    "```math\n",
    "\\iota(x)=(x\\mod M_0, \\lfloor\\frac{x}{M_0}\\rfloor\\mod M_1,\\dots, \\lfloor\\frac{x}{M_0\\cdot\\dots M_{n-1}}\\rfloor\\mod M_n)\n",
    "```\n",
    "For layout $L = (2,(2,2)):(4,(2,1))$. \n",
    "\n",
    "$L(1) = L(1, (0, 0))$ and \n",
    "\n",
    "$L(4) = L(4\\mod 2, \\frac{4}{2}\\mod 2, \\frac{4}{2\\times 2}\\mod 2) = L(0, (0, 1))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea4f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fL(0) = 0\n",
      "fL(1) = 4\n",
      "fL(2) = 2\n",
      "fL(3) = 6\n",
      "fL(4) = 1\n",
      "fL(5) = 5\n",
      "fL(6) = 3\n",
      "fL(7) = 7\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def create_layout2():\n",
    "    S = (2, (2, 2))\n",
    "    D = (4, (2, 1))\n",
    "    layout = cute.make_layout(S, stride=D)\n",
    "    for i in cutlass.range_constexpr(cute.size(S)):\n",
    "        cute.printf(\"fL({}) = {}\", i, layout((i)))\n",
    "create_layout2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cac61b",
   "metadata": {},
   "source": [
    "## Tensor Slice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd42ce0",
   "metadata": {},
   "source": [
    "## Elementwise Add kernel\n",
    "Now we have a basic understanding on the CuTe layout. Let see how we can use it in writing kernels. \n",
    "\n",
    "Note that CuTe is just an abstraction in programming not in mental model. To get the peak performance, we still need to know in detail how GPU works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1389793",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.kernel\n",
    "def naive_elementwise_add_kernel(\n",
    "    gA: cute.Tensor,\n",
    "    gB: cute.Tensor,\n",
    "    gC: cute.Tensor,\n",
    "):\n",
    "    # This kernel perform gC = gA + gB\n",
    "    tidx, _, _ = cute.arch.thread_idx()\n",
    "    bidx, _, _ = cute.arch.block_idx()\n",
    "    bdim, _, _ = cute.arch.block_dim()\n",
    "\n",
    "    thread_idx = bidx * bdim + tidx\n",
    "\n",
    "    # Map thread index to logical index of input tensor\n",
    "    m, n = gA.shape\n",
    "    ni = thread_idx % n\n",
    "    mi = thread_idx // n\n",
    "\n",
    "    # Map logical index to physical address via tensor layout\n",
    "    a_val = gA[mi, ni]\n",
    "    b_val = gB[mi, ni]\n",
    "\n",
    "    # Perform element-wise addition\n",
    "    gC[mi, ni] = a_val + b_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8674ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.jit  # Just-in-time compilation decorator\n",
    "def naive_elementwise_add(\n",
    "    mA: cute.Tensor,  # Input tensor A\n",
    "    mB: cute.Tensor,  # Input tensor B\n",
    "    mC: cute.Tensor,  # Output tensor C\n",
    "):\n",
    "    num_threads_per_block = 256\n",
    "\n",
    "    m, n = mA.shape\n",
    "\n",
    "    kernel = naive_elementwise_add_kernel(mA, mB, mC)\n",
    "\n",
    "    kernel.launch(\n",
    "        grid=((m * n) // num_threads_per_block, 1, 1),  # Number of blocks in x,y,z\n",
    "        block=(num_threads_per_block, 1, 1),  # Threads per block in x,y,z\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d709803",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = 16384, 8192\n",
    "\n",
    "a = torch.randn(M, N, device=\"cuda\", dtype=torch.bfloat16)  \n",
    "b = torch.randn(M, N, device=\"cuda\", dtype=torch.bfloat16)  \n",
    "c = torch.zeros(M, N, device=\"cuda\", dtype=torch.bfloat16) \n",
    "\n",
    "num_elements = sum([a.numel(), b.numel(), c.numel()])\n",
    "\n",
    "a_ = from_dlpack(a, assumed_align=16)  \n",
    "b_ = from_dlpack(b, assumed_align=16)  \n",
    "c_ = from_dlpack(c, assumed_align=16) \n",
    "\n",
    "naive_elementwise_add_ = cute.compile(naive_elementwise_add, a_, b_, c_)\n",
    "\n",
    "naive_elementwise_add_(a_, b_, c_)\n",
    "\n",
    "torch.testing.assert_close(c, a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff388a0",
   "metadata": {},
   "source": [
    "In previous kernel we only load one element of A and B per thread, in practice that is not gonna saturated memory bandwidth. We need to perform vectorized load for greater utilization of memory bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b2919",
   "metadata": {},
   "source": [
    "Using ``tiled_tensor = cute.zipped_divide(tensor, tiler)``, we can partition the input\n",
    "``tensor`` into groups of ``tiler`` blocks. For vectorization, we specify ``tiler``\n",
    "as the block of data each thread accesses (4 contiguous elements in the same row, or ``(1,4)``).\n",
    "Different threads can then access different blocks by indexing into the 2nd mode of ``tiled_tensor``.\n",
    "\n",
    "```python\n",
    "mA : cute.Tensor                           # (2048,2048):(2048,1)\n",
    "gA = cute.zipped_divide(a, tiler=(1, 4))   # tiled/vectorized => ((1,4),(2048,512)):((0,1),(2048,4))\n",
    "```\n",
    "\n",
    "$\n",
    "    \\begin{array}{ccccc}\n",
    "    & ((1,4) & , & (2048,512)) & : ((0,1),(2048,4)) \\\\\n",
    "    & \\underbrace{\\phantom{(1,4)}}_{tiler} & & \\underbrace{\\phantom{(2048,512)}}_{threads} & \\\\\n",
    "    & \\text{\\scriptsize per-thread} & & \\text{\\scriptsize num of tiles}\n",
    "    \\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eefbe58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.kernel\n",
    "def vectorized_elementwise_add_kernel(\n",
    "    gA: cute.Tensor,\n",
    "    gB: cute.Tensor,\n",
    "    gC: cute.Tensor,\n",
    "):\n",
    "    tidx, _, _ = cute.arch.thread_idx()\n",
    "    bidx, _, _ = cute.arch.block_idx()\n",
    "    bdim, _, _ = cute.arch.block_dim()\n",
    "\n",
    "    thread_idx = bidx * bdim + tidx\n",
    "\n",
    "    # Map thread index to logical index of input tensor in unit of vector\n",
    "    m, n = gA.shape[1]  # thread-domain\n",
    "    ni = thread_idx % n\n",
    "    mi = thread_idx // n\n",
    "\n",
    "    # Map logical index to physical address via tensor layout\n",
    "    a_val = gA[(None, (mi, ni))].load()     # a_val.shape = (1, 4)\n",
    "    b_val = gB[(None, (mi, ni))].load()\n",
    "    print(f\"[DSL INFO] sliced gA = {gA[(None, (mi, ni))]}\")\n",
    "    print(f\"[DSL INFO] sliced gB = {gB[(None, (mi, ni))]}\")\n",
    "\n",
    "    # Perform element-wise addition\n",
    "    gC[(None, (mi, ni))] = a_val + b_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb8e18",
   "metadata": {},
   "source": [
    "This vectorized kernel follows a similar structure to its naive non-vectorized counterpart,\n",
    "with one key difference: the tensor slicing pattern. By using `(None, (mi, ni))` as the slice indices,\n",
    "we can extract a `(1,4)` sub-tensor from `gA`, `gB` and `gC` like \n",
    "\n",
    "$ gA[(None, (mi, ni))]: $\n",
    "\n",
    "$\n",
    "  \\begin{array}{ccccc}\n",
    "    Layout: & ( & (1,4)                        & , & (2048,512) & )                    & : & ((0,1),(2048,4)) & \\xrightarrow{\\text{slice}} & ((1,4)):((0,1)) \\\\\n",
    "            &   & \\underbrace{\\phantom{(1,4)}} &   & \\underbrace{\\phantom{(2048,512)}} &   & \\\\\n",
    "    Coord:  & ( & None                         & , & (mi, ni)   & )                    &   &\n",
    "  \\end{array}\n",
    "$\n",
    "\n",
    "Then tensor data can be loaded into vector via the `gA[(None, (mi, ni))].load()` method. It is equivalent to\n",
    "\n",
    "```python\n",
    "v0 = gA[(0, (mi, ni))]   # => mA[(mi, ni * 4 + 0)]\n",
    "v1 = gA[(1, (mi, ni))]   # => mA[(mi, ni * 4 + 1)]\n",
    "v2 = gA[(2, (mi, ni))]   # => mA[(mi, ni * 4 + 2)]\n",
    "v3 = gA[(3, (mi, ni))]   # => mA[(mi, ni * 4 + 3)]\n",
    "```\n",
    "\n",
    "### Assumed Alignment\n",
    "\n",
    "In order to guide compile to use vectorized load/store, we must tell compiler to assume alignment of incoming pointer. \n",
    "It's on users side to guarantee actual pointer at runtime meet the alignment restriction.\n",
    "\n",
    "```python\n",
    "a_ = from_dlpack(a, assumed_align=16)\n",
    "b_ = from_dlpack(b, assumed_align=16)\n",
    "c_ = from_dlpack(c, assumed_align=16)\n",
    "\n",
    "# Compile kernel with alignment assumption\n",
    "compiled_func = cute.compile(vectorized_elementwise_add, a_, b_, c_)\n",
    "```\n",
    "\n",
    "It's worth to note that partitioned or tiled tensor could have different alignment of its base pointer because of offset\n",
    "during sub-slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ae59a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DSL INFO] Tiled Tensors:\n",
      "[DSL INFO]   gA = tensor<ptr<bf16, gmem, align<16>> o ((1,4),(16384,2048)):((0,1),(8192,4))>\n",
      "[DSL INFO]   gB = tensor<ptr<bf16, gmem, align<16>> o ((1,4),(16384,2048)):((0,1),(8192,4))>\n",
      "[DSL INFO]   gC = tensor<ptr<bf16, gmem, align<16>> o ((1,4),(16384,2048)):((0,1),(8192,4))>\n",
      "[DSL INFO] sliced gA = tensor<ptr<bf16, gmem, align<8>> o ((1,4)):((0,1))>\n",
      "[DSL INFO] sliced gB = tensor<ptr<bf16, gmem, align<8>> o ((1,4)):((0,1))>\n"
     ]
    }
   ],
   "source": [
    "@cute.jit\n",
    "def vectorized_elementwise_add(mA: cute.Tensor, mB: cute.Tensor, mC: cute.Tensor):\n",
    "    threads_per_block = 256\n",
    "\n",
    "    gA = cute.zipped_divide(mA, (1, 4))\n",
    "    gB = cute.zipped_divide(mB, (1, 4))\n",
    "    gC = cute.zipped_divide(mC, (1, 4))\n",
    "\n",
    "    print(\"[DSL INFO] Tiled Tensors:\")\n",
    "    print(f\"[DSL INFO]   gA = {gA}\")\n",
    "    print(f\"[DSL INFO]   gB = {gB}\")\n",
    "    print(f\"[DSL INFO]   gC = {gC}\")\n",
    "\n",
    "    vectorized_elementwise_add_kernel(gA, gB, gC).launch(\n",
    "        grid=(cute.size(gC, mode=[1]) // threads_per_block, 1, 1),\n",
    "        block=(threads_per_block, 1, 1),\n",
    "    )\n",
    "\n",
    "a = torch.randn(M, N, device=\"cuda\", dtype=torch.bfloat16)\n",
    "b = torch.randn(M, N, device=\"cuda\", dtype=torch.bfloat16)\n",
    "c = torch.zeros(M, N, device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "a_ = from_dlpack(a, assumed_align=16)\n",
    "b_ = from_dlpack(b, assumed_align=16)\n",
    "c_ = from_dlpack(c, assumed_align=16)\n",
    "\n",
    "compiled_func = cute.compile(vectorized_elementwise_add, a_, b_, c_)\n",
    "compiled_func(a_, b_, c_)\n",
    "\n",
    "# verify correctness\n",
    "torch.testing.assert_close(c, a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6a418",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. CuteDSL elementwise add education [notebook](https://github.com/NVIDIA/cutlass/blob/main/examples/python/CuTeDSL/notebooks/elementwise_add.ipynb) \n",
    "1. Simon's blog in cutedsl [this](https://veitner.bearblog.dev/bridging-math-and-code-cute-layout-algebra-in-cutedsl/), [this](https://veitner.bearblog.dev/thread-value-layouts-in-cute/) and [this](https://veitner.bearblog.dev/an-applied-introduction-to-cutedsl/)\n",
    "2. cutlass official tutorial on [layout](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/01_layout.md) and [layout algebra](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d3f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cute-Tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
